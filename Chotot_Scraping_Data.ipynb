{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links from mogi.vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mogi.vn/quan-6/mua-can-ho-chung-cu/ban-can-ho-2pn-1wc-tai-western-capital-q-6-nhan-nha-o-ngay-gia1ty950-id22342298\n",
      "https://mogi.vn/quan-6/mua-can-ho-chung-cu/ban-can-3pn-2bc-90m2-western-capital-q-6-3-ty070-vao-o-ngay-id22295691\n",
      "https://mogi.vn/quan-tan-binh/mua-can-ho-chung-cu/canho-hang-sang-doi-dien-coopmart-thang-loi-830tr-can-so-rieng-trondoi-id22211739\n",
      "https://mogi.vn/quan-tan-phu/mua-can-ho-chung-cu/can-ho-le-trong-tan-820tr-2pn-so-rieng-tron-doi-tang-full-nt-moi-id22268671\n",
      "https://mogi.vn/quan-2/mua-can-ho-chung-cu/quan-2-chinh-chu-can-ban-gap-can-ho-ngan-hang-ho-tro-vay-100-id22343672\n",
      "https://mogi.vn/quan-8/mua-can-ho-chung-cu/ban-can-ho-cc-avila-114-an-duong-vuong-p-16-q-8-113m2-id22292283\n",
      "https://mogi.vn/quan-10/mua-can-ho-chung-cu/ban-can-ho-chung-cu-ha-do-86m2-2pn-full-nt-moi-100-id21474089\n",
      "https://mogi.vn/quan-2/mua-can-ho-chung-cu/quan-2-chinh-chu-can-ban-gap-can-ho-ngan-hang-ho-tro-vay-100-id22343673\n",
      "https://mogi.vn/quan-6/mua-can-ho-chung-cu/can-ho-dep-lucky-palace-77m2-co-so-hong-rieng-dia-chi-50-pha-id22351981\n",
      "https://mogi.vn/quan-2/mua-can-ho-chung-cu/quan-2-chinh-chu-can-ban-gap-can-ho-ngan-hang-ho-tro-vay-100-id22343675\n",
      "https://mogi.vn/quan-10/mua-can-ho-chung-cu/can-ban-can-ho-rivera-park-thanh-thai-phuong-14-quan-10-hcmc-dien-id22351996\n",
      "https://mogi.vn/quan-10/mua-can-ho-chung-cu/thi-truong-ban-8-5-ty-rieng-can-nay-chi-7-3-ty-id22335812\n",
      "https://mogi.vn/quan-2/mua-can-ho-chung-cu/quan-2-chinh-chu-can-ban-gap-can-ho-ngan-hang-ho-tro-vay-100-id22343676\n",
      "https://mogi.vn/huyen-nha-be/mua-can-ho-chung-cu/can-ho-so-hong-3pn-chung-cu-dragon-hill-phu-long-view-dep-3ty1-id21921727\n",
      "https://mogi.vn/quan-2/mua-can-ho-chung-cu/quan-2-chinh-chu-can-ban-gap-can-ho-ngan-hang-ho-tro-vay-100-id22343677\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "arr = []\n",
    "timeout_limit = 5  # Giới hạn timeout là 5 giây\n",
    "\n",
    "for page in range(1):\n",
    "    page = str(page + 1)\n",
    "    link = \"https://mogi.vn/mua-can-ho-chung-cu?cp=\" + page\n",
    "    session = HTMLSession()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    try:\n",
    "        r = session.get(link,headers=headers, timeout=5)  # Timeout ở đây là 5 giây\n",
    "    except Timeout:\n",
    "        print(f\"Timeout occurred for page {page}. Skipping...\")\n",
    "        continue  # Bỏ qua trang nếu có timeout\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    a = soup.find_all(\"a\", class_='link-overlay')\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        link = a[i]['href']\n",
    "        print(link)\n",
    "        arr.append(link)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(arr, columns=['links'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Xuất file csv\n",
    "df.to_csv('test1.csv',index=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "arr = []\n",
    "timeout_limit = 5  # Giới hạn timeout là 5 giây\n",
    "\n",
    "# Open a file for writing results\n",
    "with open('collected_links.txt', 'w', encoding='utf-8') as file:\n",
    "\n",
    "    for page in range(2000):\n",
    "        page = str(page + 1)\n",
    "        link = \"https://mogi.vn/mua-can-ho-chung-cu?cp=\" + page\n",
    "        session = HTMLSession()\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "        \n",
    "        try:\n",
    "            r = session.get(link, headers=headers, timeout=5)  # Timeout ở đây là 5 giây\n",
    "        except Timeout:\n",
    "            print(f\"Timeout occurred for page {page}. Skipping...\")\n",
    "            continue  # Bỏ qua trang nếu có timeout\n",
    "        \n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        a = soup.find_all(\"a\", class_='link-overlay')\n",
    "        \n",
    "        for i in range(len(a)):\n",
    "            link = a[i]['href']\n",
    "            print(link)\n",
    "            arr.append(link)\n",
    "            # Write the link to the file\n",
    "            file.write(link + '\\n')\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "# Create a DataFrame from the collected links\n",
    "df = pd.DataFrame(arr, columns=['links'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('collected_links.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web cần tải\n",
    "url = \"My URL\"\n",
    "\n",
    "# Gửi HTTP GET request để tải nội dung của trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem request có thành công hay không (status code 200 là thành công)\n",
    "if response.status_code == 200:\n",
    "    # Sử dụng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm thẻ <div> với class là \"address\"\n",
    "    address_div = soup.find('div', class_='address')\n",
    "    \n",
    "    # Kiểm tra xem thẻ address_div có tồn tại hay không\n",
    "    if address_div:\n",
    "        # Lấy nội dung bên trong thẻ div\n",
    "        address_text = address_div.get_text()\n",
    "        \n",
    "        # Tách thông tin dựa vào dấu \"-\"\n",
    "        parts = address_text.split('-')\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            # Lấy phần sau dấu \"-\" và loại bỏ khoảng trắng ở đầu và cuối chuỗi\n",
    "            address_info = parts[1].strip()\n",
    "            \n",
    "            # Tách thông tin chi tiết dựa vào dấu \",\"\n",
    "            address_details = [detail.strip() for detail in address_info.split(',')]\n",
    "            \n",
    "            # In thông tin lọc ra từ địa chỉ\n",
    "            for detail in address_details:\n",
    "                print(detail)\n",
    "    \n",
    "    else:\n",
    "        print(\"Không tìm thấy thẻ div có class là 'address'\")\n",
    "else:\n",
    "    print(\"Lỗi trong quá trình tải trang web\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loại hình: Căn hộ\n",
      "Tình trạng pháp lý: Giấy phép xây dựng\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web cần tải\n",
    "url = \"My URL\"\n",
    "\n",
    "# Gửi HTTP GET request để tải nội dung của trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem request có thành công hay không (status code 200 là thành công)\n",
    "if response.status_code == 200:\n",
    "    # Sử dụng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm thẻ <div> với class là \"product-attributes\"\n",
    "    attributes_div = soup.find('div', class_='product-attributes')\n",
    "    \n",
    "    # Kiểm tra xem thẻ attributes_div có tồn tại hay không\n",
    "    if attributes_div:\n",
    "        # Tạo một từ điển để lưu thông tin lọc ra\n",
    "        attributes_info = {}\n",
    "        \n",
    "        # Lặp qua các thẻ con bên trong thẻ attributes_div\n",
    "        for item_div in attributes_div.find_all('div', class_='product-attributes--item'):\n",
    "            spans = item_div.find_all('span')\n",
    "            \n",
    "            # Kiểm tra xem có đúng 2 thẻ <span> trong mỗi item_div\n",
    "            if len(spans) == 2:\n",
    "                key = spans[0].text.strip()\n",
    "                value = spans[1].text.strip()\n",
    "                attributes_info[key] = value\n",
    "        \n",
    "        # In thông tin lọc ra\n",
    "        for key, value in attributes_info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Không tìm thấy thẻ div có class là 'product-attributes'\")\n",
    "else:\n",
    "    print(\"Lỗi trong quá trình tải trang web\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(link):\n",
    "    session = HTMLSession()\n",
    "    url =link\n",
    "    table_class = \".md-8\"\n",
    "    #r = requests.get(url)\n",
    "    #if len(r.html.find(\".notfound-message\"))==0:\n",
    "    r = session.get(url)\n",
    "    if len(r.html.find(\".notfound-message\"))==0:\n",
    "        arr=[]\n",
    "        html_text = r.text\n",
    "        r_html = HTML(html = html_text)\n",
    "        r_table = r_html.find(table_class)\n",
    "        if len(r_table) > 0:\n",
    "            test = r_table[0]\n",
    "            inf = test.find('.col-xs-12') \n",
    "            price = test.find('.priceWrapper___3ls0c')\n",
    "            temp = r_html.find('a')\n",
    "            col = temp[6].find('b') \n",
    "            info= inf[1].text+'\\n' \n",
    "            arr.append(info) # dia chi nha\n",
    "            arr.append(inf[3].text) # thong tin can nha\n",
    "            arr.append(price[1].text) # gia nha\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    str1 = arr\n",
    "    str1=[arr]\n",
    "    temp = arr[1].split('\\n')\n",
    "    idx=[]\n",
    "    idx.append([arr[0].split('\\n')[0]])\n",
    "    for i in temp:\n",
    "        idx.append(i.split('\\n'))\n",
    "        idx.append([arr[2]])\n",
    "    return idx     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogi address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Võ Thị Sáu, Phường Quyết Thắng, TP. Biên Hoà, Đồng Nai\n",
      "TP. Biên Hoà\n",
      "Phường Quyết Thắng\n",
      "Võ Thị Sáu\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web cần tải\n",
    "url = \"https://mogi.vn/tp-bien-hoa/mua-can-ho-chung-cu/can-ho-pegasus-day-du-noi-that-2-phong-ngu-so-hong-rieng-chi-1-8-ty-id22115063\"\n",
    "\n",
    "# Gửi HTTP GET request để tải nội dung của trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem request có thành công hay không (status code 200 là thành công)\n",
    "if response.status_code == 200:\n",
    "    # Sử dụng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    address = \"\"\n",
    "    address_details = []\n",
    "    # Tìm thẻ <div> với class là \"address\"\n",
    "    address_div = soup.find('div', class_='address')\n",
    "    if address_div:\n",
    "        print(address_div.get_text())\n",
    "        address = address_div.get_text()\n",
    "        address_details = [detail.strip() for detail in address.split(',')]\n",
    "        address_details.reverse()\n",
    "\n",
    "        for i in address_details:\n",
    "                text = i\n",
    "                if \"Đường\" in text or text == address_details[-1]:\n",
    "                    street = text\n",
    "                    print(street)\n",
    "                elif \"Phường\" in text or \"Xã\" in text:\n",
    "                    ward = text\n",
    "                    print(ward)\n",
    "                elif \"Quận\" in text or \"Huyện\" in text or \"Thành Phố\" in text:\n",
    "                    district = text\n",
    "                    print(district)\n",
    "                elif \"TP\" in text:\n",
    "                    city = text\n",
    "                    print(city)\n",
    "    else:\n",
    "        print(\"Không tìm thấy thẻ div có class là 'address'\")\n",
    "else:\n",
    "    print(\"Lỗi trong quá trình tải trang web\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
